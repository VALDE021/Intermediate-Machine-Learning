{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d59875-10ee-4dd5-8692-f7fe441b5547",
   "metadata": {},
   "source": [
    "# <u><center> Image Classification (Core)\n",
    "* Authored by: Eric N. Valdez\n",
    "* Date: 03-22-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fcf5bb-3305-4f64-8604-a62ef105c3b2",
   "metadata": {},
   "source": [
    "# Assignment:\n",
    "- ## For this assignment, you will classify chest X-rays as \"normal,\" \"pneumonia,\" or \"covid\" using Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73741c-fdb2-4aef-a93b-4b6e68cd0200",
   "metadata": {},
   "source": [
    "# <u>Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55792b71-0dc4-4e8f-b8b6-ea7040da44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(42)\n",
    "# Set the seed for TensorFlow\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import os, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import visualkeras as vk\n",
    "\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperParameters as hp\n",
    "\n",
    "folder = 'KerasTuner/'\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ba883b3-b758-47a5-8fe7-c7483663ac68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa6105-d3dd-40a2-bd62-d8b88bea780e",
   "metadata": {},
   "source": [
    "# <u>Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427d8486-df94-4a1b-b04f-f51f47006b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_history(history, figsize=(6,12), marker='o'):\n",
    "       \n",
    "    # Get list of metrics from history\n",
    "    metrics = [c for c in history.history if not c.startswith('val_')]\n",
    "    \n",
    "    ## Separate row for each metric\n",
    "    fig, axes = plt.subplots(nrows=len(metrics),figsize=figsize)\n",
    "    \n",
    "    # For each metric\n",
    "    for i, metric_name in enumerate(metrics):\n",
    "    \n",
    "        # Get the axis for the current metric\n",
    "        ax = axes[i]\n",
    "    \n",
    "        # Get metric from history.history\n",
    "        metric_values = history.history[metric_name]\n",
    "        # Get epochs from history\n",
    "        epochs = history.epoch\n",
    "    \n",
    "        # Plot the training metric\n",
    "        ax.plot(epochs, metric_values, label=metric_name, marker=marker)\n",
    "    \n",
    "        ## Check if val_{metric} exists. if so, plot:\n",
    "        val_metric_name = f\"val_{metric_name}\"\n",
    "        if val_metric_name in history.history:\n",
    "            # Get validation values and plot\n",
    "            metric_values = history.history[val_metric_name]\n",
    "            ax.plot(epochs,metric_values,label=val_metric_name, marker=marker)\n",
    "    \n",
    "        # Final subplot adjustments \n",
    "        ax.legend()\n",
    "        ax.set_title(metric_name)\n",
    "    fig.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "def get_true_pred_labels(model,ds):\n",
    "    \"\"\"Gets the labels and predicted probabilities from a Tensorflow model and Dataset object.\n",
    "    Adapted from source: https://stackoverflow.com/questions/66386561/keras-classification-report-accuracy-is-different-between-model-predict-accurac\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "    \n",
    "    # Loop through the dataset as a numpy iterator\n",
    "    for images, labels in ds.as_numpy_iterator():\n",
    "        \n",
    "        # Get prediction with batch_size=1\n",
    "        y_probs = model.predict(images, batch_size=1, verbose=0)\n",
    "        # Combine previous labels/preds with new labels/preds\n",
    "        y_true.extend(labels)\n",
    "        y_pred_probs.extend(y_probs)\n",
    "    ## Convert the lists to arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    \n",
    "    return y_true, y_pred_probs\n",
    "\n",
    "def convert_y_to_sklearn_classes(y, verbose=False):\n",
    "    # If already one-dimension\n",
    "    if np.ndim(y)==1:\n",
    "        if verbose:\n",
    "            print(\"- y is 1D, using it as-is.\")\n",
    "        return y\n",
    "        \n",
    "    # If 2 dimensions with more than 1 column:\n",
    "    elif y.shape[1]>1:\n",
    "        if verbose:\n",
    "            print(\"- y is 2D with >1 column. Using argmax for metrics.\")   \n",
    "        return np.argmax(y, axis=1)\n",
    "    \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"y is 2D with 1 column. Using round for metrics.\")\n",
    "        return np.round(y).flatten().astype(int)\n",
    "\n",
    "## PREVIOUS CLASSIFICATION_METRICS FUNCTION FROM INTRO TO ML\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def classification_metrics(y_true, y_pred, label='',\n",
    "                           output_dict=False, figsize=(8,4),\n",
    "                           normalize='true', cmap='Blues',\n",
    "                           colorbar=False,values_format=\".2f\",\n",
    "                           class_labels=None):\n",
    "    \"\"\"Modified version of classification metrics function from Intro to Machine Learning.\n",
    "    Updates:\n",
    "    - Reversed raw counts confusion matrix cmap  (so darker==more).\n",
    "    - Added arg for normalized confusion matrix values_format\n",
    "    \"\"\"\n",
    "    # Get the classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "    \n",
    "    ## Print header and report\n",
    "    header = \"-\"*70\n",
    "    print(header, f\" Classification Metrics: {label}\", header, sep='\\n')\n",
    "    print(report)\n",
    "    \n",
    "    ## CONFUSION MATRICES SUBPLOTS\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "    \n",
    "    # Create a confusion matrix  of raw counts (left subplot)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=None, \n",
    "                                            cmap='gist_gray_r',# Updated cmap\n",
    "                                            values_format=\"d\", \n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[0],\n",
    "                                           display_labels=class_labels);\n",
    "    axes[0].set_title(\"Raw Counts\")\n",
    "    \n",
    "    # Create a confusion matrix with the data with normalize argument \n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=normalize,\n",
    "                                            cmap=cmap, \n",
    "                                            values_format=values_format, #New arg\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[1],\n",
    "                                            display_labels=class_labels);\n",
    "    axes[1].set_title(\"Normalized Confusion Matrix\")\n",
    "    \n",
    "    # Adjust layout and show figure\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return dictionary of classification_report\n",
    "    if output_dict==True:\n",
    "        report_dict = classification_report(y_true, y_pred, output_dict=True, target_names=class_labels)\n",
    "        return report_dict\n",
    "\n",
    "def evaluate_classification_network(model, \n",
    "                                    X_train=None, y_train=None, \n",
    "                                    X_test=None, y_test=None,\n",
    "                                    history=None, history_figsize=(6,6),\n",
    "                                    figsize=(6,4), normalize='true',\n",
    "                                    output_dict = False,\n",
    "                                    cmap_train='Blues',\n",
    "                                    cmap_test=\"Reds\",\n",
    "                                    values_format=\".2f\", \n",
    "                                    colorbar=False,\n",
    "                                    class_labels=None):\n",
    "    \"\"\"Evaluates a neural network classification task using either\n",
    "    separate X and y arrays or a tensorflow Dataset\n",
    "    \n",
    "    Data Args:\n",
    "        X_train (array, or Dataset)\n",
    "        y_train (array, or None if using a Dataset\n",
    "        X_test (array, or Dataset)\n",
    "        y_test (array, or None if using a Dataset)\n",
    "        history (history object)\n",
    "        \"\"\"\n",
    "    # Plot history, if provided\n",
    "    if history is not None:\n",
    "        plot_history(history, figsize=history_figsize)\n",
    "    ## Adding a Print Header\n",
    "    print(\"\\n\"+'='*80)\n",
    "    print('- Evaluating Network...')\n",
    "    print('='*80)\n",
    "    ## TRAINING DATA EVALUATION\n",
    "    # check if X_train was provided\n",
    "    if X_train is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_train,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_train, y_train_pred = get_true_pred_labels(model, X_train)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_train_pred = model.predict(X_train)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_train = convert_y_to_sklearn_classes(y_train)\n",
    "        y_train_pred = convert_y_to_sklearn_classes(y_train_pred)\n",
    "        \n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_train = classification_metrics(y_train, y_train_pred, \n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_train,\n",
    "                                               values_format=values_format,\n",
    "                                         label='Training Data',\n",
    "                                              class_labels=class_labels)\n",
    "        \n",
    "    \n",
    "\n",
    "    ## TEST DATA EVALUATION\n",
    "    # check if X_test was provided\n",
    "    if X_test is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_test,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_test, y_test_pred = get_true_pred_labels(model, X_test)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_test_pred = model.predict(X_test)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_test = convert_y_to_sklearn_classes(y_test)\n",
    "        y_test_pred = convert_y_to_sklearn_classes(y_test_pred)\n",
    "        \n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_test = classification_metrics(y_test, y_test_pred, \n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_test,\n",
    "                                              values_format=values_format,\n",
    "                                         label='Test Data',\n",
    "                                             class_labels=class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf9fa00-002e-4fe2-b92c-973908168722",
   "metadata": {},
   "source": [
    "# <u>Making the TensorFlow Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b0ea31-818a-4712-92db-353a0a086784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training folders'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'testing folders'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# examine folders \n",
    "folder = 'Data/xrays/covid'\n",
    "\n",
    "train_folders = glob.glob(folder + 'train/*')\n",
    "test_folders = glob.glob(folder + 'test/*')\n",
    "display('training folders', train_folders)\n",
    "print()\n",
    "display('testing folders', test_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdec9b37-b0a2-4b58-9c05-7eaf58bf3234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'covid']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the list of folders in the data_dir\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dfd7f22-5061-44a0-b331-037bb321ddec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "531"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of all img file paths\n",
    "img_files = glob.glob(data_dir+\"**/*\")\n",
    "len(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee1c79e5-6ca7-4c20-8ba6-161dca05e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving image params as vars for reuse\n",
    "batch_size = 32\n",
    "img_height = 96\n",
    "img_width = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55b009f9-9b08-4eaf-84ea-bb8a2c605ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 495 files belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 96, 96, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the dataset from the main folder of images\n",
    "ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "    shuffle=True,\n",
    "    label_mode='categorical',\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda12604-2bc2-4534-b5ea-ac0e3de06ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine number of batches in dataset\n",
    "ds_size = len(ds)\n",
    "ds_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74f791-b55d-4c0c-8c4e-47ac0471d202",
   "metadata": {},
   "source": [
    "## `Saving Class Info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3799111c-7ca8-484c-a85c-854f2c738836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the class names for later use\n",
    "class_names = ds.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2696a190-b1d4-4518-92a0-2248f79856a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving # of classes for later use\n",
    "num_classes = len(class_names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a98b3ed-e574-40eb-ab65-0a2d8e7ba036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '.ipynb_checkpoints'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving dictionary of integer:string labels\n",
    "class_dict = dict(zip(range(num_classes), class_names))\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b26f1-c838-4242-b0e3-511a35f63296",
   "metadata": {},
   "source": [
    "## `Split the dataset into a training-validation-test split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eabb7142-7120-4912-9da7-037fd9d379f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 0 batches as training data\n",
      "Use 0 batches as validation data\n",
      "The remaining 1 batches will be used as test data.\n"
     ]
    }
   ],
   "source": [
    "# Set the ratio of the train, validation, test split\n",
    "split_train = 0.7\n",
    "split_val = 0.2\n",
    "split_test = .1 \n",
    "# Calculate the number of batches for training and validation data \n",
    "n_train_batches =  int(ds_size * split_train)\n",
    "n_val_batches = int(ds_size * split_val)\n",
    "print(f\"Use {n_train_batches} batches as training data\")\n",
    "print(f\"Use {n_val_batches} batches as validation data\")\n",
    "print(f\"The remaining {len(ds)- (n_train_batches+n_val_batches)} batches will be used as test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1847b3f8-5180-4ad2-8aff-382625b4a67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set is 0 batches long.\n"
     ]
    }
   ],
   "source": [
    "# Use .take to slice out the number of batches \n",
    "train_ds = ds.take(n_train_batches)\n",
    "# Confirm the length of the training set\n",
    "print(f'The training set is {len(train_ds)} batches long.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b41978de-5c17-4b61-b554-f88b84aa0e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation set is 0 batches long.\n"
     ]
    }
   ],
   "source": [
    "# Skipover the training batches\n",
    "val_ds = ds.skip(n_train_batches)\n",
    "# Take the correct number of validation batches\n",
    "val_ds = val_ds.take(n_val_batches)\n",
    "# Confirm the length of the validation set\n",
    "print(f'The validation set is {len(val_ds)} batches long.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28fb00dc-b578-499f-8491-57b911c11622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing set is 1 batches long.\n"
     ]
    }
   ],
   "source": [
    "# Skip over all of the training + validation batches\n",
    "test_ds = ds.skip(n_train_batches + n_val_batches)\n",
    "# Confirm the length of the testing data\n",
    "print(f'The testing set is {len(test_ds)} batches long.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7d8aa-c1e1-406c-9a46-1f8c63223731",
   "metadata": {},
   "source": [
    "# <u> `Optimize the Dataset`\n",
    "- ## Add a shuffle step to the training dataset\n",
    "- ## Add caching and prefetching to all 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb8093ff-3dfd-4726-8bc0-7af197c03970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f3e7e1c-4430-418c-b7e4-123a040083b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ShuffleDatasetV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} buffer_size must be greater than zero. [Op:ShuffleDatasetV3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m AUTOTUNE \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Optimize training data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mAUTOTUNE)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Optimize validation data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m val_ds\u001b[38;5;241m.\u001b[39mcache()\u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mAUTOTUNE)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1448\u001b[0m, in \u001b[0;36mDatasetV2.shuffle\u001b[1;34m(self, buffer_size, seed, reshuffle_each_iteration, name)\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshuffle\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1380\u001b[0m             buffer_size,\n\u001b[0;32m   1381\u001b[0m             seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1382\u001b[0m             reshuffle_each_iteration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1383\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Randomly shuffles the elements of this dataset.\u001b[39;00m\n\u001b[0;32m   1385\u001b[0m \n\u001b[0;32m   1386\u001b[0m \u001b[38;5;124;03m  This dataset fills a buffer with `buffer_size` elements, then randomly\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03m    A new `Dataset` with the transformation applied as described above.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshuffle_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m   1449\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreshuffle_each_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\shuffle_op.py:31\u001b[0m, in \u001b[0;36m_shuffle\u001b[1;34m(input_dataset, buffer_size, seed, reshuffle_each_iteration, name)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_shuffle\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     input_dataset,\n\u001b[0;32m     27\u001b[0m     buffer_size,\n\u001b[0;32m     28\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     29\u001b[0m     reshuffle_each_iteration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     30\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 31\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ShuffleDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreshuffle_each_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\shuffle_op.py:56\u001b[0m, in \u001b[0;36m_ShuffleDataset.__init__\u001b[1;34m(self, input_dataset, buffer_size, seed, reshuffle_each_iteration, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (tf2\u001b[38;5;241m.\u001b[39menabled() \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     (context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function())):\n\u001b[1;32m---> 56\u001b[0m   variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mshuffle_dataset_v3(\n\u001b[0;32m     57\u001b[0m       input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m     58\u001b[0m       buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_size,\n\u001b[0;32m     59\u001b[0m       seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seed,\n\u001b[0;32m     60\u001b[0m       seed2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seed2,\n\u001b[0;32m     61\u001b[0m       seed_generator\u001b[38;5;241m=\u001b[39mgen_dataset_ops\u001b[38;5;241m.\u001b[39mdummy_seed_generator(),\n\u001b[0;32m     62\u001b[0m       reshuffle_each_iteration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reshuffle_each_iteration,\n\u001b[0;32m     63\u001b[0m       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m   variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mshuffle_dataset(\n\u001b[0;32m     66\u001b[0m       input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m     67\u001b[0m       buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m       reshuffle_each_iteration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reshuffle_each_iteration,\n\u001b[0;32m     71\u001b[0m       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:7284\u001b[0m, in \u001b[0;36mshuffle_dataset_v3\u001b[1;34m(input_dataset, buffer_size, seed, seed2, seed_generator, output_types, output_shapes, reshuffle_each_iteration, metadata, name)\u001b[0m\n\u001b[0;32m   7282\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   7283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 7284\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7285\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   7286\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7261\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7262\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ShuffleDatasetV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} buffer_size must be greater than zero. [Op:ShuffleDatasetV3]"
     ]
    }
   ],
   "source": [
    "# Use autotune to automatically determine best buffer sizes \n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Optimize training data\n",
    "train_ds = train_ds.cache().shuffle(buffer_size= len(train_ds),\n",
    "                                   seed=42).prefetch(buffer_size=AUTOTUNE)\n",
    "# Optimize validation data\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# Optimize teset data\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3236e3-1d52-45e1-a310-c7dfe7dca0e2",
   "metadata": {},
   "source": [
    "#  `Preview the Data and Save the Shape:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f03e2c9f-647e-4de9-8292-e0a1c2486f68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get image sizes for later use\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m example_batch_imgs,example_batch_y\u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mget_single_element()\n\u001b[0;32m      3\u001b[0m example_batch_imgs\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "# Get image sizes for later use\n",
    "example_batch_imgs,example_batch_y= ds.take(1).get_single_element()\n",
    "example_batch_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a1af59f-1ebc-472a-a120-d76dc6ab67c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_batch_imgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Individual image shape\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m \u001b[43mexample_batch_imgs\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      3\u001b[0m input_shape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'example_batch_imgs' is not defined"
     ]
    }
   ],
   "source": [
    "# Individual image shape\n",
    "input_shape = example_batch_imgs[0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6125dd3-99e0-48eb-ad52-fc3ceb12112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview an image from the Dataset\n",
    "array_to_img(example_batch_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d483207-83cc-45b1-be2a-7d20ae5289cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show label value\n",
    "label = class_dict[np.argmax(example_batch_y[0])]\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74ce42-6ea7-4019-b1e0-1cae6a25b9ef",
   "metadata": {},
   "source": [
    "# `Modeling`\n",
    "## <u>Modeling Steps (for every model):\n",
    "- ### **For every model**\n",
    "    - ### Fitting the Model\n",
    "        - using validation_data\n",
    "        - many epochs (20+)\n",
    "        - an EarlyStopping callback\n",
    "        - Save the training history \n",
    "- ### **Evaluating the Model:**\n",
    "    - ### Plot the training history\n",
    "    - ### Evaluating the model on the training and test data, including:\n",
    "        - Sckit-learning confusion matrix\n",
    "        - Sckit-learning classification Report\n",
    "        - The results from model.evaluate method\n",
    "            - (`Tip`: Use your custom flexible evaluation functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481679c-5539-4a31-b4f0-9d6bb51c9f7c",
   "metadata": {},
   "source": [
    "## `1) Build a Simple CNN Model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "427c53fe-0a41-45dd-a50f-0e939904eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get early stopping function that can be reused on all models\n",
    "def get_callbacks(patience=3, monitor='val_accuracy'):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor)\n",
    "    return [early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f728334-aea8-4234-b7d5-79f61472596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the building and compiling steps within a function\n",
    "def build_model():\n",
    "    # Instantatie model\n",
    "    model = models.Sequential()\n",
    "    # Scaling layer\n",
    "    scaling_layer = layers.Rescaling(1./255, input_shape=input_shape)\n",
    "    model.add(scaling_layer)\n",
    "    \n",
    "    # Convolutional layer\n",
    "    model.add(layers.Conv2D(filters=8, kernel_size=3, input_shape=input_shape, padding='same')) \n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    \n",
    "    # Convolutional layer\n",
    "    model.add(layers.Conv2D(filters=8, kernel_size=3, input_shape=input_shape, padding='same')) \n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    \n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(3, activation=\"softmax\")) \n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5a230f8-fe4b-40c8-a3f8-bb5dbf9e1911",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model1 \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Scaling layer\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m scaling_layer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mRescaling(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m\u001b[43minput_shape\u001b[49m)\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39madd(scaling_layer)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Convolutional layer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_shape' is not defined"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model1 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1d92b-c266-47c7-9c78-766934cb30f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model1.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds,\n",
    "    epochs=25,\n",
    "    callbacks=get_callbacks(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076100f-41de-4919-92b0-c9808c22446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with the CNN + Dataset\n",
    "evaluate_classification_network(model1,\n",
    "                                X_train=train_ds,\n",
    "                                X_test=test_ds,\n",
    "                                figsize=(9,9),\n",
    "                                values_format='0.1f',\n",
    "                                history=history);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1dddff-1a31-4990-a348-f0aaba0191a6",
   "metadata": {},
   "source": [
    "**`Evaluation`**\n",
    "\n",
    "- The model reached 89% accuracy, which will be a good starting point for further optimization/tuning.\n",
    "- To try and further increase performance, we will use a Dense layer, additional Conv2D layers, and try a higher number of filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2efb1-0203-4ce8-8f94-641be6ce7a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0785ad02-ab3d-4f30-a55e-f40b69c7672e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d368cda-cbb7-450e-9a01-a50a4a82e52d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66ce01-b334-46ea-a4a6-21aa55cf050b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa212f-10a8-4777-ad9e-8fd17bb1bd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b353b920-2184-4452-a98e-02641ea9830f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39117898-7dc8-4ee4-973c-33b0b201a792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
